---
title: "Lab3"
author: "Maxwell Patterson"
date: "2024-04-17"
output:
  html_document: default
  pdf_document: default
---
### Assignment Lab 3:

Due next week: April 23 at 11:59PM

For this assignment you'll use the article data you downloaded from Nexis Uni in Week 2.

```{r packages, message=FALSE}
library(quanteda)
library(tm)
library(topicmodels)
library(ldatuning)
library(tidyverse)
library(tidytext)
library(reshape2)
library(LexisNexisTools)
```

1.  Create a corpus from your articles.

```{r}
setwd("/Users/maxwellpatterson/Desktop/classes/spring/eds231/eds231-materials/Nexis/kern_oil_news")

# Reading in docx files
post_files <- list.files(pattern = ".docx",
                         path = getwd(),
                         full.names = TRUE,
                         recursive = TRUE,
                         ignore.case = TRUE)

# Use LNT to handle docs
dat <- lnt_read(post_files)

meta_df <- dat@meta
articles_df <- dat@articles
paragraphs_df <- dat@paragraphs

dat2 <- tibble(Date=meta_df$Date, Headline = meta_df$Headline, id = articles_df$ID, text = articles_df$Article)

# Cleaning up data
dat2 <- dat2 %>%
  dplyr::filter(!is.na(Headline)) %>%
  distinct(Headline, .keep_all = TRUE)

# Create corpus text from df
dat2_corpus <- corpus(dat2$text)
```


2.  Clean the data as appropriate.

```{r}
# add stop words
add_stops <- stopwords(kind = quanteda_options("language_stopwords"))

# examining tokens
# tokens(dat2_corpus)

# rm punctuation, numbers and url
toks <- tokens(dat2_corpus, remove_punct = T, remove_numbers = T, remove_url = T)

# remove stop words
tok1 <- tokens_select(toks, pattern = add_stops, selection = "remove")

# convert to lower case
dfm1 <- dfm(tok1, tolower = T)

# remove words that are included only 1 or 2 times
dfm2 <- dfm_trim(dfm1, min_docfreq = 2)

sel_idx <- slam::row_sums(dfm2) > 0
dfm <- dfm2[sel_idx, ]
```


3.  Run three models (i.e. with 3 values of k) and select the overall best value for k (the number of topics) - include some justification for your selection: theory, FindTopicsNumber() optimization metrics, interpretability, LDAvis. Select the best single value of k.

```{r}
results <- FindTopicsNumber(dfm,
                            topics = seq(from = 2,
                                         to = 20,
                                         by = 1),
                            metrics = c("CaoJuan2009", "Deveaud2014"),
                            method = "Gibbs",
                            verbose = T)

FindTopicsNumber_plot(results)
```

While these results aren't ideal to work with, I will select k = 5, since it has a high value for the Deveaud2014. However, CaoJuan2009 value we want to minimize is pretty high, but I don't want to split into too many groups just so the results are easier to interpret. There is a bit of a spike in the Deveaud2014 metric at k = 5, so I feel like that is the best one to move forward with.


```{r}
# set k value based 
k <- 5

# run model
topicModel_k5 <- LDA(dfm,
                     k,
                     method = "Gibbs",
                     control = list(iter = 1000),
                     verbose = 25
)

# get results
results <- posterior(topicModel_k5)
attributes(results)

# define matrices for interpretation
beta <- results$terms
theta <- results$topics

topics <- tidy(topicModel_k5, matrix = "beta")

# pull top terms
top_terms <- topics %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ungroup() %>% 
  arrange(topic, -beta)

top_terms
```


4.  Plot the top terms in each topic and the distribution of topics across a sample of the documents (constrained by what looks good in the plot).

```{r}
top_terms %>%
  mutate(term = reorder_within(term, beta, topic, sep = "")) %>% 
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = F) +
  facet_wrap(~topic, scales = "free_y") +
  scale_x_reordered() +
  coord_flip()
```

```{r}
# assign names to topics
topic_words <- terms(topicModel_k5, 5)
topic_names <- apply(topic_words, 2, paste, collapse = " ")

#specify # of examples to inspect
example_ids <- c(1:5)
n <- length(example_ids)

# get topic proportions from example documents
example_props <- theta[example_ids,]
colnames(example_props) <- topic_names

#combine example topics with identifiers and melt to plotting form
viz_df <- melt(cbind(data.frame(example_props),
                     document = factor(1:n),
                     variable.name = "topic",
                     id.vars = "document"))

ggplot(data = viz_df, aes(variable, value, fill = document),
       ylab = "proportion") +
  geom_bar(stat = "identity") +
  coord_flip() + 
  facet_wrap(~ document, ncol = n)
```

```{r}
library(LDAvis) #visualization
library("tsne") #matrix decomposition

svd_tsne <- function(x) tsne(svd(x)$u)

json <- createJSON(
  phi = beta,
  theta = theta,
  doc.length = rowSums(dfm),
  vocab = colnames(dfm),
  term.frequency = colSums(dfm),
  mds.method = svd_tsne,
  plot.opts = list(xlab="", ylab="")
)

serVis(json)
```


5.  Take a stab at interpreting the resulting topics. What are the key themes discussed in the articles in your data base?

The topic modeling analysis reveals several key themes in the coverage of the oil industry in Kern County, California. The articles approach the subject from multiple perspectives, including environmental and public health concerns, county court and ordinance discussions related to oil and gas, the industry's impact on Kern County communities, and regulatory matters involving the state and industry boards. The documents often interweave these themes, reflecting the interconnected nature of these topics. This makes sense, given that Kern County produces 70% of California's oil, making the impact of oil wells on local communities especially significant.

Environmental and public health issues emerge as a prominent trend, appearing substantially in all five sample documents. The articles also frequently discuss the consequences of oil operations on Kern County communities, highlighting the close relationship between health concerns and oil production in the region.

Additionally, the analysis reveals a focus on regulatory aspects, with discussions involving county courts, ordinances, and state-level industry boards. This suggests that the articles not only cover the direct impacts of the oil industry but also delve into the legal and policy frameworks that shape its operations.

The presence of these intersecting themes underscores the complex nature of the discourse surrounding oil in Kern County, where considerations of economic productivity and policy decisions are closely intertwined with concerns about environmental sustainability and community well-being. The topic model thus provides a nuanced portrait of the key issues and tensions that characterize public engagement with this critical local industry, shedding light on the multifaceted challenges and debates that surround oil production in the region.
